# -*- coding: utf-8 -*-
"""Skin-cancer-randomforest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fv-dmPzPxdvYxI5WCklR8VQwxZHvO6sb
"""

from google.colab import drive
GOOGLE_COLAB= True

path = ""
if GOOGLE_COLAB:
    from google.colab import drive, files
    drive.mount('/content/gdrive/')
    path = "/content/drive/My Drive/Colab Notebooks/"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from pandas import Series,DataFrame

# numpy, matplotlib, seaborn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
# %matplotlib inline

# machine learning
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

df = pd.read_csv("/content/gdrive/My Drive/Colab Notebooks/convertcsv-3.csv")
print(df.shape)

df.head()

X = df.drop(['_id','class','image_type','pixelsX','pixelsY','name'], axis=1)

y = df['class']

X.head()

from sklearn.model_selection import train_test_split
# implementing train-test-split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=66)

from sklearn import model_selection
# random forest model creation
rfc = RandomForestClassifier()
rfc.fit(X_train,y_train)
# predictions
rfc_predict = rfc.predict(X_test)

from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix

rfc_cv_score = cross_val_score(rfc, X, y, cv=10, scoring='roc_auc')

#col = [‘SepalLengthCm’ ,’SepalWidthCm’ ,’PetalLengthCm’ ,’PetalWidthCm’]
col = ['Age', 'gender',	'diagnosis',	'diagnosis_confirm_type', 'anatom_site_general','melanocytic']

#modelname.feature_importance_
y = estimator.feature_importances_
#plot
fig, ax = plt.subplots() 
width = 1.0 # the width of the bars 
ind = np.arange(len(y)) # the x locations for the groups
ax.barh(ind, y, width, color="blue")
ax.set_yticks(ind+width/10)
ax.set_yticklabels(col, minor=False)
plt.title('Feature importance in RandomForest Classifier')
plt.xlabel('Relative importance')
plt.ylabel('feature') 
plt.figure(figsize=(5,5))
fig.set_size_inches(6.5, 4.5, forward=True)

# testing
print("=== Classification Report ===")
print(classification_report(y_test, rfc_predict))
print('\n')
print("Acuracy Score - Random Forest: ", rfc_cv_score.mean())

from sklearn.metrics import confusion_matrix
import itertools


mat = confusion_matrix(y_test, rfc_predict)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True)
plt.title('Confusion Matrix (Testing Dataset)')

plt.xlabel('true label')
plt.ylabel('predicted label');

# training
rfc2 = RandomForestClassifier(n_estimators=600, max_depth=300, max_features='sqrt')
rfc2.fit(X_train,y_train)
rfc_predict2 = rfc.predict(X_train)
rfc_cv_score2 = cross_val_score(rfc, X, y, cv=10, scoring='roc_auc')
print("=== Confusion Matrix ===")
print(confusion_matrix(y_train, rfc_predict2))
print('\n')
print("=== Classification Report ===")
print(classification_report(y_train, rfc_predict2))
print('\n')
print("Acuracy Score - Random Forest: ", rfc_cv_score2.mean())

from sklearn.metrics import confusion_matrix
import itertools

mat = confusion_matrix(y_train, rfc_predict2)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True)
plt.title('Confusion Matrix (Training Dataset)')

plt.xlabel('true label')
plt.ylabel('predicted label');

#feat_labels = ['z', 'fiberMag_u','modelMag_g','modelMag_r','modelMag_i','modelMag_z','psfMag_u','psfMag_g','psfMag_r','psfMag_i','psfMag_z']
#feat_labels = ['z', 'fiberMag_u','fiberMag_g','fiberMag_r','fiberMag_i','fiberMag_z','U-g','u-r','U-i','U-z','G-r','G-i','G-z','R-i','R-z','I-z','Lick_CN1','Lick_CN2','Lick_Ca4227','Lick_G4300','Lick_Fe4383','Lick_Ca4455',	'Lick_Fe4531','Lick_C4668','Lick_Hb',	'Lick_Fe5015',	'Lick_Mg1',	'Lick_Mg2',	'Lick_Mgb'	,'Lick_Fe5270',	'Lick_Fe5335',	'Lick_Fe5406'	,'Lick_Fe5709',	'Lick_Fe5782'	,'Lick_NaD'	,'Lick_TiO1'	,'Lick_TiO2'	,'B&H_CNB',	'B&H_H+K',	'B&H_CaI'	,'B&H_G'	,'B&H_Hb'	,'B&H_MgG'	,'B&H_MH'	,'B&H_FC',	'B&H_NaD',	'DTT_CaII8498',	'DTT_CaII8542',	'DTT_CaII8662',	'DTT_MgI8807',	'4000Abreak',	'HKratio']
feat_labels = ['Age', 'gender',	'diagnosis',	'diagnosis_confirm_type', 'anatom_site_general','melanocytic']

# Create a random forest classifier
clf = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)

# Train the classifier
clf.fit(X_train, y_train)

# Print the name and gini importance of each feature
for feature in zip(feat_labels, clf.feature_importances_):
    print(feature)

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import accuracy_score
sfm = SelectFromModel(clf, threshold=0.001)

# Train the selector
sfm.fit(X_train, y_train)

for feature_list_index in sfm.get_support(indices=True):
    print(feat_labels[feature_list_index])

#features =['Age', 'gender',	'diagnosis',	'diagnosis_confirm_type', 'anatom_site_general','melanocytic']
features = feat_labels
importances = rfc.feature_importances_
indices = np.argsort(importances)

plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()